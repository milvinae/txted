Brian zkLend  5:33  
Hi everyone thanks for joining our ama series we're waiting for the guest to hop on we will be starting the AMA in one to two minutes please stay tuned thanks? Hey hello, Oscar. Hi, what's up? Just doing pretty good yourself.

Oskaris  9:31  
Yeah, could be better could be worse generally good.

Brian zkLend  9:36  
That's awesome. I think Jonas is having a little bit of technical difficulty for joining us for joining at the moment so right. Why not? We just give him another minute or two.

Oskaris  9:49  
There he is.

Empiric  9:50  
Yeah, I think perfect. Hello, hello. Hello. Hey, there we go. Hey,

Brian zkLend  9:56  
good for you to join us. How are you doing?

Empiric  10:01  
Great. How are you guys?

Brian zkLend  10:04  
Fantastic. Fantastic. Good to have you guys. Very excited about this AMA.

So I think I think we have a pretty good crowd here. It's between three minute pass. Sorry guys for slight delay. I think it's good for us to proceed. So hi, everyone. Good morning. Good afternoon and good evening to you all. Thank you for listening in for the Senate friends ama series. Today, we are very honoured to have Jonas and Oscar with us from empirical network, empirical in their work. It's one of the first Native start where Oracle that's been built. And it's also one of our very critical partners for CK LAN to obtain price prices, price feeds for our lending protocol. And today, we want to invite them to come speak with us to explore a little bit more about their role to start in and

what they see in terms of the development of Star net, and also the Regenesis coming up and also talk about how their Oracle is different from the traditional oracle that there are servicing the market at the moment. So without further ado, I want to pass the stage to Oscar and join us to give us a quick introduction about himself. And also just a little bit background or any incident story you want to share about yourself. Yep.

Oskaris  11:28  
Yeah, of course. Well, thank you, Brian, thanks so much for for having us. And it's been a pleasure working with you guys.

For a while now. So yeah, it's great to be great to be on here. My name is Oscar. I'm one of the cofounders of empiric.

Maybe quick background on me. Jonas and I are both from Germany originally, we both went to university in the United States. And I think we had both been thinking about blockchain for a while we both sort of worked in this space. And then when z k roll ups

sort of became more and more important, we got more and more interested in building something here. And I think our journey probably starts and we're going to tell tell the entire journey yet. I'll give Jonas a chance to introduce himself as well. But just very briefly, our I think our journey started maybe a little over a year ago now. Where we were already aware of stock where but struck that didn't even exist. It was just all stock X. And at the time, through a cryptographer that was at the university that we that we were at. And that had worked with Ellie, from stock where we got put in touch with Ellie, and we, we had a chance to talk to the software team. And they told us what would happen with Stark net, and so on that that really sparked our interest. And then Stark net sort of came into existence and accelerated quite quickly. We got very excited, we started building different things. And then we understood quite quickly that an oracle was was was very much needed. And that's how we got started. But yeah, Jonas, who are you?

Empiric  13:15  
I'm Jonas Oscar Oscars taken taken most of the most of the content, I think, sort of my background is partly in crypto and partly in building data, data infrastructure and in the web two world. And so it's been a really, really cool journey built building it on on web three building, but the data layer for Stark net, and sort of as Oscar mentioned, we came into it thinking we wanted to build infrastructure. And, and this has just been been a fantastic fit. And one one other thing I would add is from sort of six or seven months ago, when we started building, it's been really, really cool to see, you know, Stark net evolve. Cairo evolve. I mean, just yesterday, zero point 10 came out, which I think is exciting. And we have the promise of 1.0 in the future. So it's been it's been really, really just a fun time. And cool to work with, with amazing protocols like, like, like you guys to build new, new new things.

Brian zkLend  14:15  
Awesome. Thanks. Thanks, guys for the introduction and background. Other than that, say, I hear that it's your your professor or school. Put you put you in touch with Ellie from software. Are there any specific attraction that you find that basically draw you to start with before the connection was actually being established? I mean, there are lots of really thriving ecosystem guides that are coming out each day and then right, I mean, yeah, just want to understand a little bit more.

Oskaris  14:49  
Right. So I think our background was sort of the story's a little bit like this. I was working I had been working at a different blockchains shored up in London for for someone over a year at that point. And I think what was quite clear was that this technology had was still has great potential. But you're really solving some some some big problems here, you're solving the double spend problem, so on. But somehow it wasn't really living up to their promise, somehow we didn't really see it in daily life. Right, I still have to explain to my parents exactly what I do. Like, what why is it not everywhere? If it's so promising? And I think the the answer that that Jonas and I gave at the time, and that we very much now will still give is, you know, the infrastructure is just not there, or the tech is just not good enough. But at 14 TPS, you can come up with the most clever protocol designs, but at the end of the day, it's just very slow and very expensive. And so we start asking ourselves, Well, you know, if this has any merit at all, it has to be better, and how could it be better? And to us at the time, it wasn't quite clear what the answer would be. So we looked at different different approaches that people were taking, and CK Roelofs just stood out or introducing zero knowledge proofs to scale these networks. That just seemed like, like a really good idea. So we started thinking more about it. And then we got to stock where we learned more about that. And that's what got us excited. But really, I think it was this sort of the this question of, okay, how how does blockchain actually become relevant? And how does it fulfil its promise? We think it has to, it has to be better than than 14 TPS on Main net, it sort of has to be much faster and much cheaper. And once we understood it was the kebabs. We said, well, great, let's let's try to contribute and and build something that would accelerate that transition. And then we understood that that that data piece, the Oracle piece that was missing, that infrastructure was missing, and we decided to build it.

Brian zkLend  16:59  
Thank you. I mean, perfect. That's perfectly leads me to the next question about impact network. It's a seeking native Oracle. And if we try to dive into it a little bit more, what does it actually mean, compared to some of the capabilities that some of the Oracle solutions that are currently offering? And what are the pros and cons? And how do you see impairing being Native on SQL roll up would have unique advantages. On top of, for example, higher TPS, or some of the other more obvious benefits that starting at it's able to provide?

Empiric  17:40  
Yeah, great, great question. I think the sort of the high level, the first thing that we started with is, you know, on the layer one, with with high gas fees, what you're forced to do and Oracle's that have been operating there, sort of through defy summer and the explosion of gas fees that came in with it actually moved a lot of their core infrastructure off chain, because even simple things like computing, a moving average, was just prohibitively expensive on the layer one and so on the layer two, that all changes and what is the K native Oracle looks like in the first instance, is sort of, very, basically you take the brain's the core of, of the Oracle. And instead of having that run off chain with, with some nodes, in sort of a more of a trusted black box setup, where you're able to do is you're able to bring that entirely on chain, and so it becomes transparent, it becomes verifiable. And so what what, just like, you know, Argent and Braavos, our smart contract based wallets, what we're able to build as a smart, contract based oracle that has its, you know, signature verification, outlier detection, aggregation, all of these sort of components, these these algorithms that make up the Oracle, the core of it, those are all on chain, those are all smart contracts. And so anyone can see the data that that is published, and anyone can see how that ends up aggregated into a robust price feed. So that's kind of the first thing. The second thing that we did is we went and said, Okay, where we're sort of the cutting edge of the future of Oracle's, where do they get their data from? Where should we get our data from? And so there's this big distinction between third party Oracle's that get their data, sort of from other node node providers, people that pull data out usually from public API's versus first party Oracle's where first party articles work directly with with data sources that have their own proprietary data and get get the data from them. And so we said, clearly, the future here is a first party article, it's sort of strictly more more robust because rather than relying you know, on the source plus an intermediary that funnels that data, you're relying only on the source, so you're just cutting out one step. And that also is great for transparency, because I'd much rather see you know, the price data that Gemini or

FTX think that the price of an asset is signed by them directly on chain, then signed by some intermediary supposedly pulled from from a public API. So I think those are the first two things that we started with is completely on chain and first party. And sort of the next thing that that I don't want to go too much into. But just just to mention, it is sort of using then verifiable computation to go beyond price feeds and build what we call computational feats. But I think I think we should get a chance to touch on that later. Just Just to mention it. I think those are sort of the three three big things.

Brian zkLend  20:36  
Thank you trying to Yeah, definitely, we will circle back on the first first party, Oracle versus third party Oracle, much more in depth. But understood when you mentioned some of the capability that start where it is able to unlock with starting that right now. I mean, start where it's launching it recursive pools and also having to Regenesis going forward. Does, does these two changes have any impact to your change to your does it open up to the capabilities sporting parent as a whole?

Empiric  21:08  
Absolutely, I think the two big things here that are sort of coming in the stark net roadmap are recursive proofs, and then off chain data availability. So recursive proofs, what we will use that for is computing more complicated computational metrics. So taking in, you know, not say, hundreds or 1000s of Feltz data points, but but two or three orders of magnitude more, and then using that to say, you know, calculate, multi multi month moving moving averages, or sort of asset correlation matrices. So a bunch of this, this advanced stuff, recursive proofs are absolutely necessary for because doing it straight up on the L two, you know, either you'd run into the step size limit that we have today on transactions, or even if that was increased, it would just be prohibitively expensive, even on the L two. And then the next thing that is coming soon, and it's quite exciting for us, is already exists on Stark x, but does not exist on Stark net, which is the ability to store data on our off off the one and only store commitments to it on the one. So what that will allow us to do is to have much, much more data, that without paying for higher call data costs by having that data be persisted to the L one, but instead having a data availability committee that that provides that stores and provides that data, you still get similar security guarantees, because you store a commitment to that data on on chain on the other one, but you don't have to store the entire data set on the

Brian zkLend  22:52  
network. That's very, very interesting. When do you think these all these changes would start to take place with some of the new upgrades that you have? Would this be something in a museum in the new near future in the next one, three months? Or is it going to be sometime in the next six to 12 months period?

Empiric  23:15  
Great question. More the former I think so. Recursive proofs. I think it sort of was 0.0 point. I think it was 9.1. They had that on the prover side, they were using some recursion to parallelize proving. The big question is when do we get a verifier written in Cairo for L threes? I know there are a bunch of great teams working on that. So I think that's my estimate is that's a question of for a few months there. And then validity mode, roll up sort of data availability off chain, the the estimate that I've heard from from the stark word team, there is sort of q4 this year. There's some interaction between that and and Cairo versions. But I think all of this my expectation is that all of this will happen in the next sort of 334 months.

Brian zkLend  24:12  
Very, very exciting time. Thank you very much for the answer. Now Okay, switching back to the first and third party Oracle questions I think one of our team first meet you guys back in Amsterdam, you guys are in stealth mode. And I think the mode of operation actually changed a little bit from the initial plan from maybe a third party Oracle because you guys wanted to speed at the time to now maybe going more full mode into a first party Oracle for everyone listening just I think you dive into that a little bit join us but could you just summarise Bobby The difference between the two Oracle's will be their pros and cons. And what it's imperative thesis for pushing on to a first part of Oracle as a long term solution.

Oskaris  24:59  
Right. So good question. I think high level yeah, as you described, you have first party and third party. Now the first party Oracle gets its data directly from from data sources. So the people who sort of where the data is created, really, and those are only two categories or types of people, it's one exchanges, exchanges, have the view cross all trades, they know exactly wherever, where the price is sort of settling. And then you have traders, of course, that trade they know at what price they they sold or bought an acid. Now, the thing about traders is, for an Oracle, the individual trader is not as interesting, you want someone who is continuously trading an asset and those traders we call market makers, right, market makers are always continuously trading at the market price. And so they have really good price information, too. They don't see the entire market base see across markets. So where the exchange sees across all trades, the market maker, if he's on many different markets, he sees across exchanges. So that is the first party Oracle set up just to have discovered briefly, the third party, Oracle is an oracle that goes to aggregators, or to third parties that don't actually originate the data. So people like coin Gecko, or coin market share those or, or public API's in general, those are sort of the sources that a third party Oracle would use. Now, the big, the big advantage of the third party approach is that it's very, very easy and very quick. Right? Brian, if you tell me tomorrow, I want to have you know, today, you have no, no oracle at all. And tomorrow, you need some sort of solution, you can immediately go and access some some public API, some some coin Gecko data or something like that. And you can sort of hack together your own Oracle. So it's very, very quick. On the other side, the first party approach, and where you get data from the data sources, that's a very heavy lift. And so that's sort of the sort of his disadvantage, where it takes a long time to actually convince someone like Gemini, like a big, big Exchange, or someone like chain street or Almeida, really big market makers to provide data to you. Right. Now, in the beginning, we were faced with sort of this decision, what would we do? And relatively early on, we decided to sort of go take the hard the hard route here and go with the first party approach, right, and convince all these people. Now why would we do that? Right, if it's much harder? The reason is, those data sources, those first party data sources, have proprietary data that is more precise and more robust than just public API's. Right? Public API's can go down or can be quite unreliable and high stress situations. And let me remind you, just quickly, high stress situations are sort of important situations for the Oracle, right, a good Oracle you recognise when it is very reliable in in very tumultuous kind of market conditions. And so we said, if we want, if we want to be that, we have to go sort of very good data with first party data. And I still remember, in the very beginning, you know, we're telling people this plan, and we tell them, you know, we told where we Columbia, and so on. And everyone said, Well, you know, we're not quite sure whether you can actually get these guys on board. And now, you know, obviously, we're quite happy to report back to you, after, you know, maybe a quarter or too many months of trying, we've gotten a phenomenal group of of data partners on our side. So the people that we work most closely with are Jane Street, Alameda research, Gemini flow traders, FTX, CMT, and a couple of others. And so with each of them, we have agreements with them, for them to sign their proprietary data. But they're private keys and post that data directly onto our smart contracts, which, which allows us to provide this ecosystem with the most reliable and best data that is out there. It's sort of data that comes directly from the source. So yeah, that's what I would say on that front. Brian, in case you're talking, you are muted.

Brian zkLend  29:46  
Yes. I have been talking for the fat path. Sorry, sorry. No, no problem. No problem. Yeah. Thanks for Thanks for reminding me. So I'll just say a quick follow up on this one. Amazing work that you guys did and You guys are having a lot of progress in terms of having the big traders and big market makers coming on board and providing data is one one question that I have. It's, for example, if we want if you want to, or if we want to have price data for some of the smaller tokens, smaller all coins, especially the ones that say, layer two, some of these exchanges may not have them readily available just yet. What will be your approach? Or will be kind of data solutions that you guys might have or already have for for these type of? Properties?

Empiric  30:41  
Great question. Yeah. I think here, sort of the the, the fundamentals that we would go back to, is thinking through, okay, where does this market data actually exist? Right? Where is demand and supply for this acid being matched? And where it meets obviously, is the market data that that protocols like, like SQL and are interested in? And so frequently for the longer tail? The answer is actually it's on chain, it's it's indexes. And so there, I think that we're working on is, at the moment is getting data from from Texas and supplementing our centralised exchange data, which is obviously the highest quality most liquid data for for sort of the biggest assets with that long tail from from Texas. So we're working on integrating both with dexus on on Stark net itself for some of the L two specific tokens that you mentioned. But we're also working on bridging over data from uniswap. Another amm is on the other one, which obviously by default, sort of will have more more liquidity, at least at the beginning. And so there are a few different sort of technical approaches to that including storage proofs and L one L two messaging. But sort of the the upshot for protocols is, you'll be able to access longer tail data from from where that data exists, which is, which is Texas, and those will all be integrated into empiric. And just like we do with centralised exchanges, will integrate many of them. And so you end up with with a robust single price feed that aggregates all that data and is updated frequently. And then this robust.

Brian zkLend  32:27  
Yep, yep, I hear you. Thank you so much for the answer. And yeah, I think having the two approaches basically covers all the assets, that's going to be in question or some of the default protocol needs. And I think that's ultimately what we do have protocol builders are most concerned about and being able to have reliable data, quick data, and then also being able to kind of understand the sources that have transparency on that. On data, I mean, data data provided inside your tank, better ready. But for the integration side, for start net, for example, we have been working with you guys and understand that you guys are also working with other different protocols on providing data feeds, and also some other services as well. You guys have been in the ecosystem very early having a lot of really strong relationship already, for example, like curb zero, you finance a lot more. Do you want to elaborate a little bit? Just talk a little a little bit more on some of the collaborations that you have? Or specifically going into? Because the reason I highlight her zero? And also Yeah, it's because I think these two, they actually request a little bit more than price or recall. And I think that's really interesting. Just the brand of the breadth of service that you guys are able to provide for developer building sounds dotnet.

Oskaris  33:57  
Right. I'll talk a little bit to it. And then I'm sure John is also has more to say on this. Generally, you know what the question that everyone's always asking us is, why the heck do I need a separate Oracle for physique ever ops? Like what why don't we just go someone who's who's already established on some other environment? And the answer is quite simple. Once you cave roll ups, you can do things that you couldn't do before. And so then the question always is, well, what is that? That thing that you couldn't do before? What are you talking about? And you're you're putting your finger on it? The answer is, lies with these collaborations that we've been doing, or these sort of more specialised fields that we've created. So this is something that Jonas has has mentioned before. This is about what we call computational fields. So on Zika Rob's the thing, the things that change, or of course, you have higher TPS and you've lowered gas. But very importantly, you get the ability to do verifiable computation. Now verifiable computation, when you use it for an Oracle is actually quite powerful, because it allows you to, to now provide these computational feeds to give this more colour, what you know what could be a computational and what could a computational Phoebe, computational feed could give you volatility of the market could give you correlation between different assets could tell you something about yield could tell you something about risk, all of those things, namely, volatility correlation and yield, or extremely important in traditional markets, what are things that we have not seen at all in Defy? The reason for that is we were not able to do any computations, any real computations on the one because it was too expensive. Now, here we can now, to give this more colour and give you good examples, and you actually mentioned some of them with curves, zero curves, zero is saying, I want to, I want to, to offer long term fixed rate loans, right, fixed rate loans are something that are very difficult to do on the one and having compound are struggling quite a bit with that. And we talked to cripsy, early on, and they said, Well, if you can tell us what the yield curve looks like, what a crypto native yield curve looks like, we can price our loans much more accurately. And so in collaboration with them, we have created a yield curve with them. That basically uses Bitcoin spot and futures prices to build a crypto native yield curve. By the way, this is the first crypto native yield curve that is basically computed computed on chain all the time that we are pioneering here, that yield curve allows curve zero to price their loans more more accurately. So that's something that we're working on with them there, I think with Yagi, and Jonas can can speak more to this as well, with Yagi. What we're working on is a market volatility feed that tells you currently the market is much more volatile than it was you know, a week ago a month ago, and so on. What I'll just say generally is on z k, Rob's you get this, this, this this great ability to create more interesting feeds and how we see price feeds is as building blocks, think of Aetherium or if Aetherium or Bitcoin or any other price feed as a building block. Now, if you compute them in different ways, you derive different insights. And that might, you know, that might be volatility, that might be yield, that might be correlation. And all of those things will be will be useful to different protocols, and will help us and will help defy to reach the next level of sophistication. Right now the products that we have are, of course, very simple. If we use those tools correctly, we can actually offer much smarter, smarter products to the consumers.

Empiric  38:01  
That's that's that's a great overview, I think the two things I would add is one thing that I touched on earlier, which is sort of back to the the technical implementation layer, right, some of these, as Oscar said, advanced computational feeds will do directly on on the L two. So that's what we've done so far with, with with the yield curve, but some of the stuff is actually going to have to move off and L three is the perfect, perfect fit for that, where what we're able to do is we're able to say, you know, off chain, create a proof for a programme, a Kira programme, that, you know, takes in some some data, validates that data, and then turns that data into something much, much more useful, whether that's, you know, a correlation matrix or a volatility, theatre, or whatever, and does that at a fraction of the cost of running that computation on DL two. So that's kind of kind of one thing that that's quite exciting in terms of unique ability to do that on a z k roll up. And I think for now to do that on Spark net. And then the other thing I would mention is sort of the macro view here. And one of the reasons why we're excited about this in terms of where the fires going, I think is is in part sort of, you know, a big push towards capital efficiency and trying to think through, you know, as as you guys have, as well, how do we, you know, keep safety guarantees, and make sure we don't have, you know, cascading liquidations and a bunch of sort of downside scenarios, but also do that without requiring you know, heavy heavy over collateralization. And so that's a fine balance, but it's a balance that you're much much better able to strike if you know more about what's going on in the markets than a simple price. And so that's I think where the fire is going and and that's, I think something that is unique to Zeke a roll ups is being able to build that that data data layer For those advanced metrics,

Brian zkLend  40:03  
that's very good point that yes, both magic excellent points. And I think I can speak for ourselves. For us at Scikit. Learn, I think, a lot of what we have on drawing boards, it may not actually come out to fruition because of the infrastructure, it's which is lacking and some of the computation, it's not practical to be done. Previously, I think, right now, it's a very exciting moment for us too, because I think we see a lot of infrastructure piece coming on board, and also their capabilities, like imperatives, improving and also expanding very rapidly. So the possibility of how we constrict your market complexity into a product, it's basically just accelerating. So yeah, so it's, I agree, it's very exciting. And also, you guys are kind of the enabling piece for a lot of the defi protocols building, I'd start that. Moving on to the next one. Next mode of questions that I want to ask it a little bit more on the risk management component, because, obviously, we have seen out there a lot of hacks, for example, bridges and appraisal records, definitely something that it's a point of weakness for a lot of different protocols for people to gain a firewall exploit. On that note, I just want to see, what are some of the common risks that in Oracle design that you guys should you guys are aware of and have designed against, or protect device users to put a barrier of insurance for them? For you for using it, then? Are there any risk mitigations that you have in place and the protocol in terms of basically controlling some of these controllable or uncontrollable risks?

Oskaris  41:58  
Right, I think you're putting your finger on something very important here. With the Oracle, you know what, when you guys, for example, when Zika lymphocytes should to use an Oracle, what you guys are really concerned about is all the risks, right? Like, sort of, on any given day with the Oracle works, we're not gonna, you know, we're not gonna, you know, celebrate that the Oracle or the quality of the Oracle is really shown by afterwards for light body, right. And sort of that is sort of characterised by its robustness against attacks and against sort of rapid market movements or quite intense market situations. Now, on the on the first on the former, what do you what do you usually see as 90% of Oracle attacks happen? In the Data sourcing, as we call it, so that happens on the data publishing side, where you know, if you look historically at PIF, or or other Oracle's when they went down, or when they reported inaccurate prices, it was usually due to a lack of decentralisation when it comes to data or like, in other words, they just had very few sources reporting. And so how we protect against that is that we, we were, we put quite a bit of work into establishing our relationships with our data partners, and making sure that we have many different ones that all have high quality data. So if any, given one of those somehow fails, or somehow becomes a bad actor, by aggregating in a certain way, and right now we're just taking a simple median, which turned out to be sort of the most effective method. By doing that, we filter out any outliers, any manipulated data, any bad data. So that's already a good a good step here. Now, what you also see with Oracle's is that sometimes in the data transmission, right from source to Oracle, you have problems. And so you know, when you think about chain link chain link ones had a, sort of a big Miss reporting of prices, that was due to problems and that were, some of the nodes didn't report correctly. How we protect against that is that we simply don't have notes. We were able, because we aren't CK Rob's we were able to get rid of all node infrastructure that might sit off chain, the only thing that happens is that the data sources post their data directly onto our smart contract. Traditionally, what would happen is, you know, some node would go and ask the data source, what their data is, and then bring that on chain for them, or also aggregate sort of off chain before they bring anything on chain. And so because we can skip that step. We are more robust in that way as well. Now beyond that, a couple of things that we're doing to increase robustness even further, or the following one thing and Jonas mentioned this previously, one thing is bringing lunch Same data over from the one. So use our one, two L two messaging to, for example, bring uniswap price data, we can use that maybe not as the immediate direct price that we give to CK lens, because it's maybe not as interesting because it's a T whop. But we can use it as sort of a sanity check. And as a price of last resort, as you can imagine, a an on chain Dex price is much more uncorrelated to the off chain, centralised exchanges than they are amongst themselves. So that's one thing we're doing. Another thing we're doing is usually when you implement an Oracle, and you know, Z Kelland, is also doing this, of course, usually, when you implement an Oracle, you always want to have a fallback Oracle. And the fallback Oracle ID you know, you may be both yourself, or you may use someone else here, we are now giving our customers we will very shortly be giving our customers the option of using a out of the box, fall back oracle that we actually didn't build. But that compound built with Coinbase. And it's called the Open Oracle project. That project was not in stark net before because there was sort of some signatures that you couldn't, that you couldn't use on stripe, we did some work to bring that over to port that over. And now we give our our customers the option to just to just use as that as a really, really good fallback option. So those are some of the things that that we're that we continue to think about. To make the Oracle even more robust. I probably missed a couple of things. So join us if there's anything I missed, feel free to add. But yeah, that's how I would answer

Empiric  46:41  
that you think you've covered covered a bunch of those, those sort of fall back. And I mean, the great thing about just to sort of go into the technical detail, the great thing about, you know, on on chain data from from indexes is that we're able to pull that completely trustless Lee right at that point, even the measurement of the price is not off chain, it's entirely on chain. And so that's, that's fantastic. But the problem of that with with that, of course, is as as Jessica mentioned to you off is sort of a lagging indicator. Because turns out, if you look at the majority of historical Oracle attacks, they've actually been manipulating the data source using things like Flash loads. And so you need to upgrade which which makes it sort of less less up to date. And so it's it's perfect as a fallback, but not as good as sort of a main person.

Brian zkLend  47:35  
Yeah, if I'm remembering paraphrase one, one point on the fallback Oracle, so that would be something that when works, for example, for us as a user of imperative network, ultimately, we can basically have two running at the same time and then basically check the open Oracle against as a fallback against the data that we get from the primary one, which is impaired. And so that will be running in parallel. That's right. Perfect. And, yeah, just just for my case, no, no, that's fine. Yeah. That went Do you think that will be coming out?

Oskaris  48:16  
Tomorrow? The code is so you can tell Jonathan? Yeah, if Jonathan is sick off code, hacking together some articles and he is he has better stuff to do, then, you know, tomorrow's his birthday. Because that's when we will, we will do that. It's just sort of the idea behind it is, right, like you can hack together your article. But But ideally, this is not something you want to do, right? Like, this is not your expertise. It's not something you want to spend time on, you want to focus on something, especially if you're you know, SQL. And of course, you guys are very advanced team, you have a lot of resources. But for other teams that are just starting out, they should they might not have the resources and so on, they still should use a fallback Oracle. Right, they still should have robustness. And so for us, this was sort of a first research project on how to bring up over the open Oracle. Now that we've done it, we want to offer that. I mean, for free, of course, just for anyone to use, just as a really good fallback that we didn't build, right. It's totally it has nothing to do with empiric. Almost, we just sort of ported it over and did some some sort of low level work of for making it useful here. And yeah, that's sort of the idea behind it.

Brian zkLend  49:25  
That's amazing. I mean, that's, that's what's great about the ecosystem is people building up free things for us to make ourselves better. And then there you go. That's one example

Oskaris  49:35  
of that. Right? Anything, anything for you, Brian.

Brian zkLend  49:39  
Perfect. Okay, so I think we are almost closing to our timespan for the AMA want to dive into a little bit on the last topic about empirical roadmaps growth plan, and what is your shared vision or ambition For empiric, on the whole secret lair or starting that as a whole.

Oskaris  50:06  
Right, I think one thing that we want to do is we want to continue to excel, as at robustness and precision. I think today, if you look at empiric, we stand here as the market leader on Stark net, I think most, most protocols are using us here. I think they do that, because people understand that, or I have seen that we are we are robust, we are precise. And we want to become even more robust and more precise. And that, you know, that works through we make that happen through the sort of the things that we described, with like, you know, l one trail to messaging, but also just really more and more data sources. So I think within the next quarter, you'll see that we will be adding other really high quality first party data sources to make the data component even stronger, is second thing we want to do is we want to expand to two other data verticals. That can include VRF, as much as it can include weather or sports data, I think a lot of the promises that we saw on the one, for example, around parametric insurance never came to fruition because you know, when you have 14 TPS, and when you're dealing with extremely high gas, and none of those protocols actually make that much sense. Now on a zero up with where those things change to the better, we think we will, we'll see a renaissance of that, we want to enable that through really good weather data, whatever it might be. So you know, you'll see new, new, new data verticals. And then thirdly, a very important thing for us is, of course, the computational piece. And I think that will be sort of incredibly important for Defy. And what we want to do is, I mean, right now we of course have the yield curve life, we will have market, volatility feeds life and correlation feeds, like, we want to build that out more. And we want to help people like you guys had CK lend, or other protocols to, for example, reach better capital efficiency, right or, or improve their protocols through better data that happens through computational data. We see it in traditional markets, and traditional finance every day. And so we want to catch up on that gap that we have to fall there. That's going to be a big focus for us. And it's really exciting, because this is something you can't do anywhere else. And I think for z k, Rob's we should always remember this is not just something where we rebuild everything that has been built before. Here, we want to try new things, and we want to build better things. And for us as an Oracle, that's about computational data. So that will be a big focus. And we'll report back soon. What that is yielding.

Brian zkLend  52:57  
Thank you so much, Oscar. All right. I think that's all all the time that we have. Unless you have any closing remarks you want to share with the audience about any alpha is about impairing or any any insights on starting a DUI chair, feel free to do so. The floor is yours. Cool. Well, first,

Empiric  53:22  
I want to say thanks. Thanks for having us. It's been it's been a great a great time. And that really, really interesting conversation and sort of more generally extension of a lot of the work we've been doing together the last few months. I think two things to mention one one Oscar touched upon as sort of a you know, if you're thinking about using Oracle's on Stark net, as Oscar mentioned, because we were live for the last last five months, we have that that that audible track record. So every time data is updated, our smart contracts emit an event and so you can use stalking that index, or any other solution does pull those and I think so far, it's actually more than than 6 million events that we've emitted. And so, you know, feel free to take take a look at that we think the data speaks for itself. But also, reach reach out to us DMS are open on on Twitter, we're in stark NET Core stars all over the place. So feel free to talk to us and then yeah, just just sort of super excited to be in this ecosystem and, and be building together. It's it's an amazing time.

Brian zkLend  54:27  
Likewise, thank you so much, man. Thank you, Oscar. Thanks for joining us. Thank you. Thank you. Amazing AMA's amazing typography guys. As a last piece of housekeeping for our audience, audience for the AMA. We do have a secret word to share for the poll app. The secret word for this AMA, it's data, da ta all in small cap, data, da ta. So there you go. Thank you so much, again, guys. Very, very helpful and very, very informative, and look forward to expanding our cooperation expanding across a whole ecosystem what you

Oskaris  55:02  
guys. Awesome. Thank you so much, Brian. Talk soon. And thank you everyone for listening. Bye bye.

Brian zkLend  55:09  
Thanks. Thanks guys. Bye


